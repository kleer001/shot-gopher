services:
  vfx-ingest:
    build:
      context: .
      args:
        # Set CUDA_ARCH env var before build to target your GPU:
        #   export CUDA_ARCH=$(./scripts/detect_cuda_arch.sh)
        #   docker compose build
        CUDA_ARCH: ${CUDA_ARCH:-7.5 8.6 8.9}
    image: vfx-ingest:latest
    container_name: vfx-ingest
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - VFX_MODELS_DIR=/models
      - VFX_PROJECTS_DIR=/workspace/projects
    volumes:
      # Model storage (read-only for safety)
      # Default: <repo>/.vfx_pipeline/models (set by run_docker.sh)
      - ${VFX_MODELS_DIR:-./.vfx_pipeline/models}:/models:ro
      # Project workspace (read-write)
      # Default: <repo>/../vfx_projects (set by run_docker.sh)
      - ${VFX_PROJECTS_DIR:-../vfx_projects}:/workspace/projects
      # Dev: mount code for live editing (no rebuild needed)
      - ./scripts:/app/scripts:ro
      - ./workflow_templates:/app/workflow_templates:ro
      - ./web:/app/web:ro
      - ./docker/entrypoint.sh:/app/entrypoint.sh:ro
      # GS-IR patches (PyTorch 2.6 weights_only fix)
      - ./.vfx_pipeline/gsir-patches/baking.py:/app/.vfx_pipeline/GS-IR/baking.py:ro
      - ./.vfx_pipeline/gsir-patches/render.py:/app/.vfx_pipeline/GS-IR/render.py:ro
    ports:
      - "8188:8188"  # ComfyUI web interface
    stdin_open: true
    tty: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
